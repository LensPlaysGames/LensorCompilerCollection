* Glint

Goals:
- One or many powerful, expressive type systems.
- A LISP without top level parentheses.
- Entirely My Own (You Can Fuck Off Please and Thank You, Don't "Help"; MAKE YOUR OWN).

** Shitty No-good Stinking Rotten Filthy Ideas

*** "disallow"
[2024-06-22 Sat 11:24]

It would be cool to be able to "disallow" specific variables beyond a
given point in a control flow. i.e. setup a few things and then make
them access-only after that. I guess this is sort of like const except
that you can reassign it up until it is disallowed.

#+begin_src
  foobar :string "";
  ;; ... do things
  foobar := "some other thing";
  (disallow foobar);
  ;; Fine
  printf "%s\n", foobar[0];
  ;; PROGRAM ERROR
  foobar := "mcgee mcgoo I lost my shoes";
#+end_src

*** Basic Outline
[2024-06-23 Sun 16:44]

Comma is an expression separator, semi-colon is a statement separator. Or something like that.

**** Line Comments
#+begin_example
;; This is a comment.
#+end_example

**** Variable Access
#+begin_example
foo;
#+end_example

**** Variable Declaration
#+begin_example
foo: [Byte];
#+end_example

**** Variable Declaration w/ Definition
#+begin_example
foo: [Byte] "ruh roh";
#+end_example

**** Variable Declaration w/ Definition (type inferred)
#+begin_example
foo :: "ruh roh";
#+end_example

**** Variable Assignment
#+begin_example
foo := "ruh roh";
#+end_example

**** Function Declaration w/ Immediate Return Expression
#+begin_example
func: [Byte]() "im the returned value when you call (func)";
#+end_example

**** Function Declaration w/ Body
#+begin_example
func1: [Byte]() {
  ;; .. some other stuff ..
  "im the returned value when you call (func1)";
};

;; Note that `return` also exists.
func2: [Byte](cond: Boolean) {
  if cond return "I might be the returned value, if cond is truthy";
  "im the returned value when you call (func2)";
};
#+end_example

**** Function Declaration (external)
#+begin_example
external puts: void(:byte.ptr);
#+end_example

Basically, if you are linking with a library written in another language, you can use =external= before declaring the Glint type information for the symbol from that library. Most of the time, =external= should be reserved to library wrappers/bindings written in Glint, and then =import='ed like everything else. NOTE: =external= automatically implies =export= ($\text{external} \Rightarrow \text{export} \land \text{external}$).

Defining a body expression for an externally-declared function is ill-formed.

**** Declarations (exported)
#+begin_example
;; Type-inferred Variable
export value :: 69;
;; Function
export function :int() 420;
#+end_example

NOTE: Only affects module compilation.

When compiling a module, the symbols are kept local by default. That is, the symbols defined in the generated object file(s) have a local scope/binding, meaning that when these objects are linked, other libraries and programs would not be able to access those symbols. If you are making a library and want to expose an API, put =export= behind the declaration to make it available for =import= in other Glint files (and usable in other languages via their external linkage procedures, assuming you line up the calling convention).

**** Function Call
#+begin_example
;; All of the following are equivalent.
func;
func();
(func);
#+end_example

**** Function Call w/ Arguments
#+begin_example
;; All of the following are equivalent.
func arg1, (arg_func) arg3;
func(arg1 (arg_func), arg3);
(func, arg1 (arg_func) arg3);
#+end_example

**** Struct Type Declaration
#+begin_example
my-struct: struct {
  my-member: long;
  some-data: int, ;; You can use commas or semi-colons, doesn't matter.
};
#+end_example

**** Enum Type Declaration
In Glint, an enum is just a way to name values.

#+begin_example
my-enum: enum {
  one :: 1,
  two :: 2,
};

names: enum [Byte] {
  JERRY :: "Jerry Seinfeld",
  JEREMY :: "Jeremy Elbertson",
};
#+end_example

If a type is declared, the values will be constrained to be convertible to that type.

If no type is declared, an enum will act like a union of all of the types of all of the values within it. Every enum has a corresponding sort of "variant index" stored inside it that may be queried to see what value the enum holds. Basically, each named value will be assigned an index within an enum, and then we will convert the name passed to the "enum.has" thing into the corresponding index value within the variant, allowing for the runtime check to be generated properly.

NOTE: I may choose to separate the "flag style" enum and the "variant style" enum into "enum" and "union", respectively.

#+begin_example
enum my-enum {
  one :: 1,
  two :: 2,
};

enum names :string {
  JERRY :: "Jerry Seinfeld",
  JEREMY :: "Jeremy Elbertson",
};

enum mix {
  STERMA :: "Jeremy CUCK-LORD Elbertson",
  PERMA :: 9001,
};

;; Declare variable `foo` of type `mix` initialized to value `mix.STERMA`.
foo :: mix.STERMA;

;; Reflection with .has function
if (foo.has STERMA)
  (print foo);
else if (foo.has PERMA)
  (print "It's over 9000!");

;; EVENTUALLY
switch foo {
  case STERMA (print foo);
  case PERMA (print "It's over 9000");
}
;; or even
(print {switch foo {
  case STERMA foo;
  case PERMA "It's over 9000";
}});
#+end_example

**** NOTE: Macros Are a Thing, but They Aren't Covered Here

*** Fuck Exceptions
[2024-06-23 Sun 17:50]

*** Strings Are Important
[2024-06-25 Tue 14:01]

#+begin_src lisp
  data :: "1abc2
  pqr3stu8vwx
  a1b2c3d4e5f
  treb7uchet";

  for byte :: data {
      byte;
  };
  ;; The above will expand according to the following macro (or very
  ;; similarly).
  macro
      for $sym :: $container:expr_once $body:expr ;
  defines i, s
  emits
      s :: $container.end;
      cfor {
          i :: container.begin;
          i < s;
          ++i;
      } {
          $sym :: $container[i];
          $body;
      };
  endmacro

  for line :: data.lines {
      for byte :: line {
          byte;
      };
  };
  ;; The above should expand to the following (roughly)
  s :: data.lines.end;
  cfor {
      i :: data.lines.begin;
      i < s;
      ++i;
  } {
      byte :: data.lines[i];
      ;; ... body from outer `for` inserted here ...
      for byte :: line {
          byte;
      };
  };
#+end_src

Note that due to deproceduring, it is possible for this for to either call functions or access values without special handling required for those cases.

*** AOC 2023 Day 1
[2024-06-25 Tue 14:22]

#+begin_src lisp
  data :: "1abc2
    pqr3stu8vwx
    a1b2c3d4e5f
    treb7uchet";

  sum :: 0;

  first-digit-present :: false;
  first-digit-value :: 0;
  last-digit-value :: 0;
  for byte :: data {
      if byte = '\n' {
          first-digit-present := false;
          line-value :: 10(first-digit-value) + last-digit-value;
          sum += line-value;
      } else if (one-of "0123456789" byte) {
          last-digit-value := byte - 48;
          if not first-digit-present {
              first-digit-present := true;
              first-digit-value := byte - 48;
          };
      };
  };
  line-value :: 10(first-digit-value) + last-digit-value;
  sum += line-value;

  sum;
#+end_src

*** Sequences
[2024-06-25 Tue 14:24]

I guess the idea is that anything iterable will fit an interface called a Sequence, and that a lot of things built-in to the language and standard library will operate on sequences. This will handle linked lists (the most important data structure, /clearly/), dynamic arrays, strings, and more.

Something important is that a user-defined type (i.e. a struct) may meet the standards of a sequence, and therefore integrate tightly with the inner workings of the language.

*** AoC 2023 Day 2 :: Data
[2024-06-25 Tue 14:38]

#+begin_src lisp
  struct Game {
      id :: 0;
      struct CubeCount {
          red :: 0;
          green :: 0;
          blue :: 0;
      };
      records: [CubeCount];
  };

  games :: ([Game]
      (Game 1
       ([CubeCount]
        (CubeCount 4 0 3)
        (CubeCount 1 2 6)
        (CubeCount 0 2 0)))
      (Game 2
       ([CubeCount]
        (CubeCount 0 2 1)
        (CubeCount 1 3 4)
        (CubeCount 0 1 1)))
      (Game 3
       ([CubeCount]
        (CubeCount 20 8 6)
        (CubeCount 4 13 5)
        (CubeCount 1 5 0)))
      (Game 4
       ([CubeCount]
        (CubeCount 3 1 6)
        (CubeCount 6 3 0)
        (CubeCount 14 3 15)))
      (Game 5
       ([CubeCount]
        (CubeCount 6 3 1)
        (CubeCount 1 2 2)))
  );

  possible_games_id_sum :: 0;
  for game :: games {
      possible :: true;
      for count :: game.records {
          if count.red > 12 or count.green > 13 or count.blue > 14 {
              possible := false;
              (break);
          }
      }
      if possible possible_games_id_sum += game.id;
  }
  possible_games_id_sum;
#+end_src

Basically, I'm trying to show here a couple things:
- Invoking a type constructs an instance of that type
- Built-in Dynamic Arrays
May we eventually have a very capable standard library that may even have a dynamic array container type that is easier to maintain and also easier to use? Yes. But that's a long ways off, and I want it to be *easy* to have a dynamic list right off the bat (otherwise implementing that standard library will be /rough/).

*** Some Types
[2024-06-25 Tue 15:07]

**** Base Built-in Types

uX where X is any decimal integer -> arbitrary-bitwidth unsigned integer
sX where X is any decimal integer -> arbitrary-bitwidth signed integer
Byte
Bool, bool, Boolean, boolean -> Bool
String, Symbol -> String -> [Byte]

**** Type Modifiers, Complex Types

[T] -> Dynamic Array of T
T() -> Function w/ Return Type T
T.ptr -> Pointer to T

*** Lexer Macros
[2024-06-25 Tue 15:22]

You know LaTeX? Yeah, like that.

**** Simple Macro
#+begin_example
macro add34 $arg1 emits
  $arg1 + 34
endmacro

add34 35
#+end_example

This defines a macro called =add34= which takes a single macro argument; *remember, macro arguments are tokens at the lexer level*, unless otherwise specified. When a macro is expanded, any uses of the macro argument in the output, in the macro expansion, will be replaced with the token passed when invoking the macro. This means the =add34 35= macro invocation expands into =35 + 34=. Notice how there is no macro left in the final code; they are purely tools to generate code, not code itself.

**** Hygienic Macros
#+begin_example
macro increment $i emits {
  a :: 0
  $i := $i + 1
} endmacro

a :: 4
b :: 8
increment a
increment b
a
#+end_example

The above will error, complaining about the macro =increment= not being hygienic in the expansion of =increment a=. That's right, no crazy macro shadowing issues like in C.

The proper way to declare a new variable within a macro is to generate a unique symbol to use by using =gensym=.
#+begin_example
macro increment $i
defines a ;; <---- the important part
emits {
  a :: 0
  $i := $i + 1
  a := 42
  $i
} endmacro

a :: 67
increment a
increment a
#+end_example

By letting the compiler know that a symbol within the macro is meant to be unique to each invocation of the macro, it may generate a unique symbol and replace uses of the original symbol with the unique one.

In case you aren't familiar: [[https://en.wikipedia.org/wiki/Hygienic_macro]]

**** Empty Macro
#+begin_example
macro foo emits endmacro
foo
#+end_example

**** Macro Selector =expr=
#+begin_example
macro capture_binary $binop:expr emits
  $binop + $binop
endmacro

1 + capture_binary 17 + 17
#+end_example

Because these macros work at the level of the lexer, it can be really difficult to work with values at the language level, at the parser level. To fix this, we have the =expr= selector. This may be applied to any macro argument, and it will change how tokens are bound to the argument. Instead of binding the argument to the first token found, it will bind the argument to the first expression parsed. As you can see above, this is useful to be able to capture values at the language level rather than individual tokens.

You may be asking, how does this even work? Well, the lexer has a token type that is a node the parser returns. We parse an expression and store the result in a token, as weird as that is.

**** Macro Selector =expr_once=
#+begin_example
variable :: 0
foo : int() {
  variable := variable + 1
  33 + variable
}

macro doubled $a:expr_once emits
  $a + $a
endmacro

doubled foo() ;; returns 68
#+end_example

Basically, if you expand a single input token multiple times in the output of a macro, /and that input token evaluates to something that has side effects/, those side effects may occur multiple times and that may not be the behavior necessary. For this, we have the =expr_once= selector. It is much like the =expr= selector, except that even if the expression is expanded multiple times, it is only ever evaluated once.

This is accomplished by caching the return value in a variable with a generated, unique symbol and then replacing all expansions of the original expression with that unique symbol.

*** Parser Macros
[2024-06-25 Tue 15:22]

If you've written macros in LISP, that is what I'm talking about.

Basically, these sort of macros will be "real code", but the catch is that it is run at compile-time and generates code itself that is then evaluated in the final code. Again, if you haven't written LISP macros this might sound confusing, but it's actually really simple (and incredibly powerful in turn) once you get the hang of it.

#+begin_src lisp
  (defmacro foo (a b c)
    (if (= a "sum")
        `(+ ,b ,c)
        `(- ,b ,c)))
#+end_src

Hopefully you can read LISP, as I don't yet have a syntax worked out in Glint.

The above defines a macro =foo= with three macro arguments, =a=, =b=, and =c=.

The macro body is an =if= control flow expression.

So, what exactly happens when we invoke the macro?
#+begin_src lisp
  (foo "difference" 70 1)
#+end_src

Well, /at compile time/, the macro body is expanded and then run, resulting, in this case, in =(- 1 70)= being returned. Now, you might be a bit weirded out at this point... the macro returned /more code/. And that's exactly the key to these types of macros: their return value is the code that they will expand into. In the final code, it is as if we erased the macro invocation and inserted the return value of that invocation in it's place.

Helpful NOTE: While quasiquoting sounds fancy and people choose to make it hard to understand (by refusing to because words are new and/or different to what they are used to, /grrr/), it is actually as simple as string interpolation.

#+begin_src lisp
(setq foo "69")
`("abc" ,foo "def")
#+end_src
The above quasiquotation with an unquote is equivalent to the following string interpolation.
#+begin_src js
const foo = "69";
`abc${foo}def`;
#+end_src

If you can't wrap your heads around quasiquoting, it's a /you/ problem :&.

*** A String Should Be Able to Be Used as a Symbol
[2024-06-25 Tue 15:50]

A =String= should be able to be used in the same ways a symbol can, or converted between the two. While this isn't possible with a String who's value is not known at compile time (unless we implement some sort of Glint environment with a runtime, but that doesn't sound like my style), it /is/ possible for values known at compile time---for example those often passed to macro invocations. The idea is that we may eventually have a parser macro that takes or makes a String, possibly does things to it, then uses that to return code that accesses a variable bound to a symbol equivalent to the contents of that String...

*** =unless= (no else allowed) and =until=
[2024-06-25 Tue 16:46]

The opposites of =if= and =while=, they're there if you want them.

=unless=
#+begin_src lisp
  success :: do_something_that_might_fail();
  unless success {
      (print "something went horribly wrong\n");
      (exit 1);
  };
#+end_src

=until=
#+begin_src lisp
  data :: "Some \"string\" returned from some C library\n";
  c_str :: data[0];
  until @c_str = '\0' {
      (print @c_str);
      c_str := c_str[1];
  };
#+end_src

*** Function Overloading
[2024-06-26 Wed 01:28]

I am not against it, but I don't often use it. It's a nice-to-have and for the amount of work it takes to implement, I'm not super focused on getting this in particular working.

*** The Focus
[2024-06-27 Thu 12:42]

I just want to clarify what should be a focus in the design of Glint: strings and dynamic arrays ("vectors"). Specifically, /native/ and /extensible/ strings and dynamic arrays, and operations on them.

In my opinion, having these two things makes implementing just about anything much, much easier and faster. At the same time, making the user implement these things themselves is one of the most common avenues to bugs, memory corruption, and more.

*** Multiple Strings? Nah
[2024-06-27 Thu 17:36]

I want multiple string literals in a row to become a single string literal.

*** Reduction Rules
[2024-07-02 Tue 10:06]

String <- String, [String];

Call F(x...) <- F x...;

*** Parser Macro Syntax Playground
[2024-07-02 Tue 10:06]

Attempt 001 (=cmacro= return type)
#+begin_src lisp
  ;; If you aren't used to parser macros, it is code that runs at compile
  ;; time to return code that will be compiled into the final program.
  ;; Think of it like code that operates on code instead of data.
  plus : cmacro(nodes : [Node]) {
    out :: (NodeBinaryAdd 0 0);
    for node :: nodes {
      out.left := node;
      out.right := (NodeBinaryAdd 0 0);
      out := out.right;
    };
    out;
  };

  (plus 34 35);
  ;; MACRO-INVOCATION plus NODES 34 35
  ;; expansion steps:
  ;; 0 + 0
  ;; 34 + 0
  ;; 34 + (0 + 0)
  ;; 34 + (35 + 0)
  ;; 34 + (35 + (0 + 0))
  ;; 34 + 35 + 0 + 0
#+end_src

While this does make =cmacro= a keyword, I really don't think that's an issue. I'm okay with a thousand keywords, honestly. The above =plus= macro doesn't require any quasiquoting or any of the more complex evaluation requirements that some parser macros have; I've also written it to be simple rather than perfectly efficient. This is just to play with the syntax, mind you.

*** =embed "foo.bin";=
[2024-07-02 Tue 10:18]

Yes, like the new =C= standards, we will have data embeds that also adhere to include paths. I'm pretty sure we'll just have this create a fixed =Byte= array, and have it able to be used in initialisers.

*** New Overview/Outline
[2024-07-03 Wed 07:47]

#+begin_example
Variable Access :: IDENTIFIER
Variable Declaration
 :: IDENTIFIER "::" init-value
 :: IDENTIFIER ":" type
 :: IDENTIFIER ":" type init-value
Variable Assignment :: IDENTIFIER ":=" new-value
Lambda :: "lambda" function-type body-expr;
Function Call :: function [ args... ];
Typed Compound Literal :: type [ args... ];
Compound Literal :: "!{" [ args... ] "}";

Control Flow CFor :: "cfor" "(" init-expr; condition-expr; iterator-expr; ")" body-expr;
Control Flow while :: "while" condition-expr body-expr;
Control Flow until :: "until" condition-expr body-expr;
Control Flow if :: "if" condition-expr body-expr;
if-else Expression :: "if" condition-expr body-expr "else" body-expr;

Binary Member Access :: "."
Binary Subscript :: array "[" index "]"
Binary Dereferencing Access :: pointer "->" IDENTIFIER
Binary Add :: "+"
Binary Sub :: "-"
Binary Mul :: "*"
Binary Div :: "/"
Binary Equals :: "="
Binary Logical And :: "and"
Binary Logical Or :: "or"
Binary Bitwise And :: "&&"
Binary Bitwise Or :: "||"
Binary Bitwise Xor :: "xor"
Binary Bitwise Shift Left :: "shl"
Binary Bitwise Shift Right, Signed :: "sar"
Binary Bitwise Shift Right, Unsigned :: "shr"

Unary Prefix Logical Not :: "not"
Unary Prefix Bitwise Not :: "~"
Unary Prefix Pointer Dereference :: "@"
Unary Prefix sizeof :: "sizeof"
Unary Prefix alignof :: "alignof"

File Embed :: "embed" file-path;

Type void
Type bool, Bool, boolean, Boolean
Type Byte
Type String, Symbol
Type Arbitrary Signed Int :: "s" INTEGER
Type Arbitrary Unsigned Int :: "u" INTEGER
Type Default Signed Int :: "int" -> ssize_t
Type Default Unsigned Int :: "uint" -> size_t
Type Modifier Array :: "[T]", "[T size]"
Type Modifier Pointer To :: "T.ptr"
Type Modifier Reference :: "T.ref"
Type Modifier Function :: "T()"

Lexer Macro
Parser Macro

IDENTIFIER :: The usual
INTEGER :: The usual
#+end_example

Ambiguities that have been resolved:
- By changing array syntax from =T[size]= to =[T size]=, subscripts of values and arrays of arbitrary types are now parseable into single trees.

I guess type casting will be equivalent to instantiating that type (compound literal), where we just call the type like it's a function, i.e. =(T v)=.

*** Lambda
[2024-07-03 Wed 09:37]

#+begin_src lisp
  (lambda T() ...body)
#+end_src

NOTE: I wonder if, with the (T v) parsing syntax, we could just have =T() body-expr= be a lambda? Not needed, and I'll start with just the lambda keyword, but it's something to think about ... when else would a function type not be followed immediately by a semi-colon? Also would make the lambda syntax the same as normal function syntax, just without the =name:= preceding it.

*** Unary Minus to Free Dynamic Array
[2024-07-03 Wed 17:26]

#+begin_src lisp
  foo : [Byte 4];
  ;; ... use foo
  -foo;
  ;; if foo is used "from here on", after the free, the compiler should
  ;; issue an error. NOTE: what if -foo is in body of while, etc.
#+end_src

Maybe also unary plus to allocate a type, i.e. =+T= in Glint -> =new T();= in C++.
#+begin_src lisp
  ;; NOTE: Type of bar is Byte.ptr (return type of allocation function).
  bar : +Byte;
  ;; ... use bar
  -bar;
#+end_src

I dunno, doesn't seem like the worst thing in the world. But it also does. Lol. Allocation will always suck, afaik.

*** Print
[2024-07-04 Thu 14:20]

#+begin_src lisp
  print 42;        ;; -> 42
  print "42";      ;; -> 42
  print (cchar 42); ;; -> *
#+end_src

EVENTUALLY we will want a way to do /something like/ string interpolation (probably generalized to work on any dynamic array, just for fun).

*** "Character"
[2024-07-04 Thu 20:10]

While we don't have a char type like C does, and our strings take advantage of both single and double quotes anyway, we do have a short-hand syntax for converting a string /literal/ into it's corresponding byte value using the =b= prefix.
#+begin_src lisp
  "hello"; ;; type of [Byte], value of "hello"
  b"hello"; ;; ERROR: too many bytes in bytestring being converted into byte value.
  b"h"; ;; type of Byte, value of 104
#+end_src

*** Small Note
[2024-07-04 Thu 20:45]

Equivalent:
#+begin_src lisp
  ;; Type-declared variable declaration initialized with integer literal.
  offset : usz 0;
  offset : usz = 0;
  ;; Type-inferenced variable declaration initialized with compound literal.
  offset :: usz 0;
#+end_src

NOTE: May end up removing ability to put ~=~ in there.

*** "Pincer" Type Inference
[2024-07-04 Thu 20:48]

NOTE: This may all be really stupid, or not novel, or something like that. That's okay.

Fuck bottom-up and top-down closed-off stupid mindsets. Like the parsers I write, both will be used in tandem. My idea is that sometimes when we are type-checking we will have an "expected type": this is the top-down approach, I think.

Type-checking the initialising expression of =offset : usz -1;=, for example, we can pretty heavily infer that the type should be a usz: we don't need the initialising expression to explicitly declare it's own type (i.e. bottom up). Not that it couldn't, of course: a value may absolutely have it's type fully defined. This is the approach of the pincer: it aims to find places where it can't pinch, and those are errors. That is, it tries to find places where the /defined type/ of an expression (bottom-up) isn't convertable to the /declared type/ of an expression (top-down); these are type errors.

Why does this matter? Well, if you have ever used C, you will recognized the declaration above and realise that, in C, as soon as you compile it, you will get a big, phat warning (provided =-Wconversion= is on, but, if it isn't, /you are insane/), or maybe even an error. This is because =-1=, as an expression by itself, is signed. I mean, if it wasn't, the sign wouldn't be doing much there, would it :Þ? So, the expression itself defines it's type to be some signed integer type (or at least constrains it so), and the declaration declares it's value to be of an unsigned integer type. As you can see, this mismatch is what causes the type warnings/errors to occur in C. However, looking at this from a philosophical point of view, what did the programmer intend in writing such syntax? THEY OBVIOUSLY INTENDED TO INITIALISE A =usz= WITH A VALUE OF =-1= AND HAVE ASKED THE LANGUAGE AND/OR COMPILER TO DO THE CONVERSION AUTOMATICALLY. Like, really, what kind of programmer is relying on a compiler to tell them they initialised something to a value that has to be converted to what they are initialising it to? I understand they ideally want no operation to happen "silently" in C or whatever but in C++... yeah, I don't know why they keep this system around. So, instead of having a perfectly-precise pincer-point that may only effectively "pinch" when they are exactly the same, I want to put some leeway in there to /trust the programmer/ and therefore bias the pincer towards the top-down declared type if present (converting the value of bottum-up defined type to the declared type if necessary). This means no error on things like =offset : usz -1=. the compiler will insert a cast/conversion to =usz=, and store that value.

C++ people may argue that this is a problem because it does conversions that might be costly very easily and invisibly, or something like that. My response to that is that if a programmer cares about performance, they will write performant code, and know how to do so. If our goal was to allow any old idiot to write perfectly fast code, then, sure, we may want to make it obvious that expensive operations occur when creating a string from a string view or something of that nature, and make it difficult to do in the first place---to try and force programmers who don't care about performance to write performant code when it may not even matter for the program they are writing. In my opinion, this is /futile/. If a programmer's program runs too slow, /they/ will figure out why and fix up that code; if that programmer was converting between string view and string all the time, or making a bunch of dynamic array copies, or whatever other awfulness they have stumbled upon, then they will learn a valuable lesson in how /not/ to write code as well as the consequences if you do.

*** Dynamic Arrays
[2024-07-06 Sat 19:59]

**** Initialisation

#+begin_src lisp
  foo: [Byte];
  ;; should be equivalent to
  foo_t : struct {
      data: Byte.ptr;
      size: int;
      capacity: int;
  };
  foo: foo_t;
  foo.size := 0;
  foo.capacity := 8;
  foo.data := malloc 8(sizeof Byte);
#+end_src

**** Appending

1. Grow if new size would exceed or equal current capacity.
2. Write appended value.
3. Update size.

#+begin_src lisp
  foo: [Byte];

  foo += 42;
  ;; should be equivalent to
  ;; (grow if needed)
  if foo.size + 1 >= foo.capacity {
      foo.capacity *= 2;
      foo.data := realloc foo.data foo.capacity;
  }
  ;; (write new value)
  @foo.data[foo.size] := 42;
  ;; (update size)
  foo.size += 1;
#+end_src

My main question right now is that I'm not entirely sure where these things should be implemented, i.e. during Parsing or during IRGen.

*** ptr ptr
[2024-07-08 Mon 23:03]

cadr cdar, ptr pptr

#+begin_src c
  argv: Byte.ptr.ptr
#+end_src

Yeah, that's not gonna fly.

#+begin_src c
  argv: Byte.pptr
#+end_src

Yeah, that makes a lot of sense.

For people who need a pointer to a pointer to a pointer... what in the fuck are you doing? Write it out if you need to be that crazy.

*** Aliasing
[2024-07-15 Mon 14:40]

Linkers are powerful tools, and most languages don't give enough syntax to take advantage of those tools, in my opinion.

This is where Glint aliases come in. An alias is a declared symbol to an already existing definition. For example, this allows giving multiple names to a single function, with different linkages (exported, internal, etc). So, a Glint library could expose =read= to Glint programs that import it, and alias =read= to =glint_read= for usage from C (that way in the C bindings you don't have to declare `read` as an external symbol, as that is likely already defined by libc or the program itself, etc). This way, we get interopability without having the verbosity of C forced into the language; we can still mangle function names, but now we have a way to provide unmangled, direct names to the linker for any given global variable or function definition.

*** The Why of Types
[2024-07-16 Tue 15:43]

- Boolean :: On or Off
- Integer :: Group of Bits
- Array :: Group of Unnamed, Contiguous Data Elements
- Enum :: Group of Named Constant Expressions
- Struct :: Group of Named, Contiguous Data
- Union :: Group of Names to Contiguous Data
- Sum :: Group of Names to Contiguous Data w/ Introspection

*** optional opt-out of sum type checked access
[2024-07-16 Tue 17:13]

*** Just want to reiterate that we should be able to turn identifiers into strings
[2024-07-17 Wed 10:59]

Especially type members (enum identifiers) given a value (i.e. nameof foo.x -> "x", or something like that).

If we included the member strings in an array, we could even use the tag value of a sum type to decide which member name to print

*** Real World Sum Type Usage Example
[2024-07-17 Wed 11:12]

#+begin_src glint-ts
  token : sum {
    invalid :cint -1;
    integer :int 0;
    identifier :[Byte] "";
    string :[Byte] "";
  };

  byte_view : struct {
    begin: byte.ptr;
    end: byte.ptr;
  };

  ;; parameter should probably be a Byte view reference type
  lex :token(input :byte_view.ref) { ... };
  parse :void(input :byte_view.ref) {
    t :: lex(input);
    if (has t.integer) ... ;; Parsed integer
    else if (has t.identifier) ... ;; Parsed identifier
    else if (has t.string) ... ;; Parsed string
  };
#+end_src

*** Supplantation
[2024-07-18 Thu 16:28]

Inheritance sucks, so, let's do something different. I have been thinking that inheritance is basically just a member whose symbols are imported into the above namespace; that is, the actual memory layout isn't different from just including the inherited type as the type of first member. The largest convenience with inheritance, compared to this "as a member" method, is that the names and identifiers to access various data can easily get long and confusing (i.e. do I need ~foo.id.name~ or ~foo.info.name~?). So, it seems that inheritance is an overboard solution for something that really just amounts to altering the valid symbols in the program.

I think a simpler way of doing this would be via supplantation. To define a member as supplanted means to replace the member with the contents of the given type. Most often this will be done with structs, to form a sort of header "base" struct with various different "derived" structs.

#+begin_src glint-ts
  vector2 : struct {
    x :int
    y :int
  };

  vector3 : struct {
    supplant foo;
    z :int;
  };

  baz :bar;
  baz.x := 69;  baz.foo.x := 69; ;; the same
  baz.y := 42;  baz.foo.y := 42; ;; the same
  baz.z := 69;
#+end_src

**** Supplanting a Sum Type Seems Interesting

#+begin_src glint-ts
  foo : sum { x :int  y :int };
  bar : struct {
      :supplant foo;
      z :int;
  };

  boz :: bar;

  ;; Actually sets foo.tag to "x" correspondent and then stores data into
  ;; foo.data
  boz.x := 42;

  ;; Codegens as an access to foo sum type (i.e. checked or unchecked)
  boz.x;

  has boz.x; ;; true
#+end_src

As you can see, this would form a very powerful foundation for header/data sort of data structures.

One thing that I want to make happen is that a pointer to a struct with supplanted members is implicitly convertible to a pointer to those supplanted member's types. This may require adding a known offset to the base pointer, so the values themselves are not equal, but they are implicitly convertible. This way you can pass a struct with a supplanted =foo= type into a function that expects a parameter of =foo= type, whether it is a value, areference, or a pointer. This means that you could pass a Vector3 with a Vector2 supplanted member directly into functions that operate on Vector2's, or even pointers or references to Vector2's.

Inspired by Pascal "with do" blocks, as well as Jai "using" keyword.

**** Reaching Downward

NOTE: I don't think this is the direction Glint should go in; I think there are other ways to implement such designs (like a ).

One thing this doesn't address (yet) is reaching from a "base" type to a "derived" type.

This would require vtables (terrible name), which is basically the act of inserting a special tag and a pointer to the "derived" type's vtable into the "base" type. The special tag will be initialized when either a "base" or "derived" type is created with a value that corresponds to the type. The pointer in the "base" type will also be initialised with a pointer to the vtable of the "derived" type.

NOTE: A really good, quick view of how vtables might work if written by hand by Jonathan Blow: https://youtu.be/ZHqFrNyLlpA?si=4uOsb2I4OTSbxcz0&t=4517.

This means that, given a runtime instance of a "base" type, we have
  A. the actual type that this type was created as a part of (i.e. the "derived" type) via the inserted special tag, and
  B. the actual place in memory where the instance of this type is located.
From this, we can, at runtime, add checks whenever we access something that may be overriden (still not sure on syntax/semantics of this) to ensure we access the proper data. I.e. if special tag is equal to the "base" type tag, then we can access things "normally", but if the special tag corresponds to a "derived" type, we go through the pointer, to the vtable, to access it.

For functions (virtual data members don't really make sense?), the resolved function only changes based on the "derived" type, aka the special tag data member in the "base" type. So, we generate global struct instances (accessible at runtime) that correspond to each type, and the elements in the struct instance are the compile-time known values of the function addresses for that particular type. So, if both "base" and "derived" types have identical implementations, both of these vtables would have equal values. But, if "derived" or "base" was to alter their (and only their) implementation, the vtable for that type would be alterred to point to that implementation instead.

However, /all/ of this is exactly that: /all/-encompassing---kind of complicated. And the only benefit is, er... what exactly is the benefit of this? I guess the main use case is to have a group of "base" types that all "do the same thing" but have different ways of doing so. For example, let's say we wanted each "thing" to print something based on it's local, custom data. So, let's say we have a =Test= "base" type with an overridable =run= member. Then, after writing two tests, =SmellTest= and =SniffTest=, that override the =run= member, we want to store a group of =Test= and then iterate over all of them, calling the =run= function on each one.

#+begin_src glint-ts
  Test : base {
    run : bool() true;
  };
  SmellTest : derived Test {
    smell :uint;

    run :bool() { return smell = 420; }

  };
  SniffTest : derived Test {
    scent :uint;

    run :bool() { return scent = 69; }
  };

  tests :[Test 2] !{ (SmellTest) (SniffTest) };
  for test :: tests, test.run;
#+end_src

**** Proposed Alternative to vtables

"Write it yourself---it's easier."

#+begin_src glint-ts
  Test : sum {
    smell :uint;
    sniff :uint;
  };
  smelltest_run :bool(test :Test) { return test.smell = 420; }
  snifftest_run :bool(test :Test) { return test.scent = 69; }
  run :bool(test :Test) {
    if (has test.smell)
      return smelltest_run test;
    if (has test.sniff),
      return snifftest_run test;
  }

  smelltest : Test !{ .smell 420 };
  snifftest : Test !{ .sniff 69 };

  tests :[Test 2] !{ (smelltest) (snifftest) };
  for test :: tests, run test;
#+end_src

If you understood the vtable stuff up above and how sum types in Glint work (tagged unions), you can pretty clearly see that this is just the user writing the vtable themselves in the form of a dispatch function =run=. At the same time, we DRASTICALLY reduced the boilerplate just defining test types over and over. The only real downside I see to this is that it's possible to add a value to the =Test= sum type (i.e. a new type of test) /without/ that type being represented in the dispatch function =run=. However, there will be solutions to this in the future (i.e. a switch/match sort-of-thing that may dispatch to code based on the stored type in a sum type which can error AT COMPILE TIME if all cases are not handled (think =-Wswitch=)). Alternatively, once we provide more introspection capabilities, we could do the C-style of doing an assert that the /count/ of types held in a sum type is a specific value; of course, this would need to be a compile-time assert, which we also need to think about, I just realised, lol.

#+begin_src glint-ts
  Test : sum {
    smell :uint;
    sniff :uint;
  };
  smelltest_run :bool(test :Test) dispatch(Test.smell run)
    test.smell = 420;
  snifftest_run :bool(test :Test) dispatch(Test.sniff run)
    test.scent = 69;

  ;; So, dispatch(Test.smell run) will cause that function to be called in a
  ;; function with an identical signature called `run` iff the parameter is
  ;; a sum type and matches the given subtype (passes `has`). Basically, the
  ;; `run` function we wrote above would be automatically generated by the
  ;; compiler. The functions are not altered in any way; the dispatch
  ;; attribute simply tells the compiler to make an entry in a separate
  ;; function that calls this function. Just syntactic candy.

  smelltest : Test !{ .smell 420 };
  snifftest : Test !{ .sniff 69 };

  tests :[Test 2] !{ (smelltest) (snifftest) };
  for test :: tests, run test;
#+end_src

#+begin_src glint-ts
  ;; Could maybe also do some syntax like this
  dispatch run :bool(t :Test) {
    smell :> smelltest_run t;
    sniff :> snifftest_run t;
  }
#+end_src

*** Calls
[2024-07-28 Sun 19:46]

#+begin_src glint-ts
  foo;     ;; parsed as access of foo, may be turned into a call through deproceduring.
  foo();   ;; parsed as call of foo
  (foo);   ;; parsed as access of foo, may be turned into a call through deproceduring.

  foo bar;     ;; parsed as call of foo, single argument of bar which may be turned into a call through deproceduring.
  bar baz;     ;; parsed as call of bar, single argument of baz which may be turned into a call through deproceduring.
  foo bar baz; ;; parsed as call of foo, two arguments, bar and baz, which may be turned into calls through deproceduring.

  ;; Notice how (bar baz) is NOT parsed as a call in the arguments, even
  ;; though it is at the top level; we are not "expecting" a function to
  ;; call so we don't assume there is one. This is akin to the "operator
  ;; position" in LISP, sort of. We can "expect another function"/put
  ;; another expression in the operator position by opening up a
  ;; parenthetical expression list.

  foo (bar baz);  ;; parsed as call of foo, single argument of call of bar, single argument of baz which may be turned into a call through deproceduring.

  ;; After we have parsed an expression in the operator position, we begin
  ;; collecting arguments into a call of that operator expression.
  ;; A comma directly following the operator position may be used to stop
  ;; this behavior (arguments will not be collected, deproceduring will
  ;; still be applied).
  ;; TODO: not implemented yet
  foo, bar baz; ;; parsed as access of foo, may be turned into a call through deproceduring, sequentially a call to bar, single argument of baz (yada yada).

  ;; however, a comma may be used to separate arguments as well (to no effect).
  foo bar, baz; ;; parsed as call of foo, two arguments, bar and baz, which may be turned into calls through deproceduring.

  ;; foo (bar, baz);
#+end_src

*** User-defined Implicit Conversions
[2024-07-30 Tue 23:19]

#+begin_src glint-ts
  MyT : struct {
    valid :bool;
    ;; ...
  };

  convert :bool(myT :MyT) myT.valid;

#+end_src

